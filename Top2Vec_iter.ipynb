{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "%pip install pandas \n",
    "%pip install top2vec\n",
    "%pip install numpy\n",
    "\n",
    "%pip install matplotlib\n",
    "%pip install umap-learn[plot]\n",
    "%pip install top2vec[sentence_encoders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 11.0.0. Please consider upgrading.\n",
      "  warnings.warn(\n",
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/umap/plot.py:203: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from read_corpus_functions import load_books_blocks_from_document, load_books_from_document, load_books_chunks_from_document, get_pos, discard_words\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from top2vec import Top2Vec\n",
    "import umap.plot\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/lem/all\"\n",
    "file_name =   \"jerome_synv9_all_blocks_without_residuals_new\" #\"jerome_synv9_id_no_names_blocks\"  \"jerome_synv9_id\" #_NOV_MEM_COL_deep_learn\n",
    "file_path = path+\"/\"+file_name + \".txt\"\n",
    "fiction = True\n",
    "txtype_select = ['NOV: próza',  'MEM: memoáry, autobiografie', 'COL: kratší próza', 'VER: poezie'] if fiction else [] #,\n",
    "folder = 'whole books  NOV COL MEM VER' if fiction else 'whole books'\n",
    "file_ending = \"_NOV_COL_MEM_VER\" if fiction else ''\n",
    "jerome_conllu =  \"data/jerome corpus/jerome_synv9_id.txt\"\n",
    "\n",
    "#books = trim_documents(books, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('ner_combined.obj', 'rb') as f:\n",
    "#        names_frame = pickle.load(f)\n",
    "#        df_ner = pd.DataFrame(names_frame)\n",
    "\n",
    "#with open('pos_NOV_COL_MEM_VER.obj', 'rb') as f:\n",
    "#        work_pos = pickle.load(f)    \n",
    "\n",
    "books, books_info = load_books_from_document(file_path, txtype_select)\n",
    "#books = discard_words(['český', 'československý', 'čestina', 'česko', 'česky'], books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(books)\n",
    "pos = get_pos( jerome_conllu, txtype_select, work_pos, df_ner)   \n",
    "pickle.dump(pos, open('pos_NOV_COL_MEM_VER.obj', 'wb'))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODES FOR VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_map_visualization_3(df):\n",
    "    languages = df.srclang.unique()\n",
    "    topics = np.unique(df.topic.unique())   \n",
    "    heat_mat = np.zeros(shape=(len(topics), 3))\n",
    "    for i, topic in enumerate(topics):\n",
    "        for j, language in enumerate(languages):\n",
    "            # print(str(topic) + ' ' + str(language) )\n",
    "            # print(len(df[(df.topic == topic) & (df.srclang == language)].index)   )\n",
    "            if language in ['cs: čeština']: #, 'en: angličtina'\n",
    "                heat_mat[i][0] = len(df[(df.topic == topic) & (df.srclang == language)].index) \n",
    "            elif language in ['en: angličtina']: #, \n",
    "                heat_mat[i][1] = len(df[(df.topic == topic) & (df.srclang == language)].index)   \n",
    "            else:\n",
    "                heat_mat[i][2] += len(df[(df.topic == topic) & (df.srclang == language)].index)     \n",
    "        heat_mat[i][0] = heat_mat[i][0] /  len(df[(df.topic == topic)])  \n",
    "        heat_mat[i][1] = heat_mat[i][1] /  len(df[(df.topic == topic)])\n",
    "        heat_mat[i][2] = heat_mat[i][2] /  len(df[(df.topic == topic)])\n",
    "    return heat_mat\n",
    "\n",
    "\n",
    "def heat_map_visualization_2(df):\n",
    "    languages = df.srclang.unique()\n",
    "    topics = np.unique(df.topic.unique()) \n",
    "    heat_mat = np.zeros(shape=(len(topics), 2))\n",
    "    for i, topic in enumerate(topics):\n",
    "        for j, language in enumerate(languages):\n",
    "            if language in ['cs: čeština']: \n",
    "                heat_mat[i][0] = len(df[(df.topic == topic) & (df.srclang == language)])   \n",
    "            else:\n",
    "                heat_mat[i][1] += len(df[(df.topic == topic) & (df.srclang == language)])    \n",
    "        heat_mat[i][1] = heat_mat[i][1] /  len(df[(df.topic == topic)])\n",
    "        heat_mat[i][0] = heat_mat[i][0] /  len(df[(df.topic == topic)])  \n",
    "    return heat_mat    \n",
    "\n",
    "\n",
    "def plot_heat_map(heat_mat, df, save_name, r, labels):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    fig = plt.imshow(heat_mat, cmap='magma_r', interpolation='nearest')\n",
    "    plt.xticks(range(r), labels)\n",
    "    num_topics = len(df.topic.unique()) \n",
    "    y_ticks = ['Topic ' + str(n) for n in range(num_topics)]\n",
    "    plt.yticks(range(num_topics), y_ticks)\n",
    "    plt.colorbar(fig)\n",
    "    plt.savefig(\"plots/whole books/top2vec/heat_map_{}_{}\".format(save_name, str(r)))\n",
    "\n",
    "\n",
    "def plot_top2vec(n_neighbors, topic_model, save_name, doc_top_reduced):\n",
    "\n",
    "    umap_args_model = {\n",
    "    \"n_neighbors\": n_neighbors,\n",
    "    \"n_components\": 2,\n",
    "    \"metric\": \"cosine\",\n",
    "    'min_dist':0.1,\n",
    "    'spread':1,\n",
    "    'random_state': 42\n",
    "    }\n",
    "\n",
    "    umap_model = umap.UMAP(**umap_args_model).fit(topic_model.document_vectors)\n",
    "    umap.plot.points(umap_model, labels=doc_top_reduced ) #\n",
    "    plt.title(save_name)\n",
    "    plt.savefig(\"plots/whole books/top2vec/plot_{}\".format(save_name)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE MODELS/ SAVE MODELS AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 15:24:29,787 - top2vec - INFO - Pre-processing documents for training\n",
      "2024-06-25 15:24:56,141 - top2vec - INFO - Creating joint document/word embedding\n",
      "2024-06-25 15:38:38,376 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2024-06-25 15:38:42,214 - top2vec - INFO - Finding dense areas of documents\n",
      "2024-06-25 15:38:42,230 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "194\n",
      "28\n",
      "278\n",
      "24\n",
      "158\n",
      "357\n",
      "108\n",
      "280\n",
      "25\n",
      "293\n",
      "170\n",
      "421\n",
      "39\n",
      "256\n",
      "255\n",
      "126\n",
      "315\n",
      "6\n",
      "323\n",
      "286\n",
      "213\n",
      "48\n",
      "22\n",
      "27\n",
      "311\n",
      "16\n",
      "429\n",
      "21\n",
      "74\n",
      "219\n",
      "73\n",
      "596\n",
      "395\n",
      "60\n",
      "353\n",
      "182\n",
      "415\n",
      "82\n",
      "58\n",
      "71\n",
      "26\n",
      "93\n",
      "75\n",
      "88\n",
      "304\n",
      "289\n",
      "9\n",
      "89\n",
      "454\n",
      "317\n",
      "308\n",
      "55\n",
      "51\n",
      "23\n",
      "407\n",
      "63\n",
      "231\n",
      "171\n",
      "209\n",
      "202\n",
      "279\n",
      "434\n",
      "426\n",
      "340\n",
      "334\n",
      "98\n",
      "223\n",
      "162\n",
      "106\n",
      "365\n",
      "379\n",
      "296\n",
      "405\n",
      "349\n",
      "300\n",
      "208\n",
      "155\n",
      "236\n",
      "492\n",
      "229\n",
      "115\n",
      "337\n",
      "284\n",
      "674\n",
      "342\n",
      "302\n",
      "151\n",
      "212\n",
      "46\n",
      "222\n",
      "78\n",
      "177\n",
      "354\n",
      "355\n",
      "348\n",
      "261\n",
      "285\n",
      "389\n",
      "399\n",
      "329\n",
      "433\n",
      "146\n",
      "200\n",
      "527\n",
      "559\n",
      "159\n",
      "276\n",
      "569\n",
      "17\n",
      "268\n",
      "598\n",
      "547\n",
      "92\n",
      "270\n",
      "316\n",
      "655\n",
      "274\n",
      "143\n",
      "335\n",
      "191\n",
      "104\n",
      "99\n",
      "164\n",
      "85\n",
      "142\n",
      "257\n",
      "273\n",
      "347\n",
      "116\n",
      "20\n",
      "368\n",
      "47\n",
      "312\n",
      "19\n",
      "320\n",
      "294\n",
      "138\n",
      "134\n",
      "301\n",
      "70\n",
      "50\n",
      "580\n",
      "103\n",
      "0\n",
      "238\n",
      "461\n",
      "438\n",
      "579\n",
      "1\n",
      "597\n",
      "234\n",
      "382\n",
      "376\n",
      "259\n",
      "341\n",
      "291\n",
      "157\n",
      "419\n",
      "331\n",
      "281\n",
      "481\n",
      "682\n",
      "533\n",
      "199\n",
      "84\n",
      "439\n",
      "168\n",
      "271\n",
      "263\n",
      "437\n",
      "563\n",
      "180\n",
      "248\n",
      "410\n",
      "137\n",
      "305\n",
      "386\n",
      "34\n",
      "411\n",
      "401\n",
      "77\n",
      "687\n",
      "297\n",
      "552\n",
      "107\n",
      "518\n",
      "113\n",
      "145\n",
      "120\n",
      "453\n",
      "128\n",
      "272\n",
      "686\n",
      "361\n",
      "443\n",
      "322\n",
      "169\n",
      "251\n",
      "307\n",
      "79\n",
      "110\n",
      "408\n",
      "587\n",
      "130\n",
      "369\n",
      "125\n",
      "117\n",
      "356\n",
      "102\n",
      "392\n",
      "396\n",
      "101\n",
      "4\n",
      "471\n",
      "153\n",
      "403\n",
      "548\n",
      "463\n",
      "184\n",
      "530\n",
      "283\n",
      "583\n",
      "613\n",
      "277\n",
      "601\n",
      "572\n",
      "183\n",
      "397\n",
      "133\n",
      "574\n",
      "582\n",
      "457\n",
      "204\n",
      "639\n",
      "477\n",
      "589\n",
      "681\n",
      "269\n",
      "599\n",
      "338\n",
      "43\n",
      "621\n",
      "602\n",
      "299\n",
      "136\n",
      "536\n",
      "398\n",
      "603\n",
      "18\n",
      "591\n",
      "373\n",
      "590\n",
      "38\n",
      "64\n",
      "467\n",
      "1\n",
      "446\n",
      "319\n",
      "139\n",
      "189\n",
      "627\n",
      "505\n",
      "543\n",
      "610\n",
      "624\n",
      "549\n",
      "608\n",
      "626\n",
      "448\n",
      "367\n",
      "571\n",
      "462\n",
      "211\n",
      "652\n",
      "646\n",
      "557\n",
      "42\n",
      "33\n",
      "10\n",
      "641\n",
      "444\n",
      "494\n",
      "555\n",
      "701\n",
      "677\n",
      "237\n",
      "609\n",
      "637\n",
      "704\n",
      "190\n",
      "653\n",
      "650\n",
      "254\n",
      "662\n",
      "452\n",
      "176\n",
      "611\n",
      "192\n",
      "414\n",
      "57\n",
      "645\n",
      "664\n",
      "45\n",
      "578\n",
      "651\n",
      "380\n",
      "692\n",
      "7\n",
      "197\n",
      "666\n",
      "225\n",
      "288\n",
      "114\n",
      "669\n",
      "466\n",
      "710\n",
      "612\n",
      "442\n",
      "80\n",
      "683\n",
      "67\n",
      "532\n",
      "535\n",
      "385\n",
      "13\n",
      "711\n",
      "150\n",
      "634\n",
      "656\n",
      "564\n",
      "570\n",
      "643\n",
      "247\n",
      "554\n",
      "673\n",
      "333\n",
      "566\n",
      "511\n",
      "654\n",
      "449\n",
      "216\n",
      "625\n",
      "593\n",
      "678\n",
      "435\n",
      "594\n",
      "551\n",
      "351\n",
      "618\n",
      "193\n",
      "221\n",
      "521\n",
      "649\n",
      "504\n",
      "8\n",
      "243\n",
      "178\n",
      "560\n",
      "631\n",
      "455\n",
      "32\n",
      "616\n",
      "460\n",
      "40\n",
      "388\n",
      "218\n",
      "122\n",
      "30\n",
      "663\n",
      "374\n",
      "166\n",
      "321\n",
      "684\n",
      "53\n",
      "240\n",
      "447\n",
      "450\n",
      "54\n",
      "49\n",
      "703\n",
      "620\n",
      "362\n",
      "528\n",
      "479\n",
      "298\n",
      "712\n",
      "640\n",
      "253\n",
      "661\n",
      "675\n",
      "558\n",
      "325\n",
      "544\n",
      "647\n",
      "694\n",
      "503\n",
      "514\n",
      "324\n",
      "715\n",
      "700\n",
      "131\n",
      "69\n",
      "436\n",
      "327\n",
      "465\n",
      "638\n",
      "215\n",
      "507\n",
      "622\n",
      "456\n",
      "619\n",
      "716\n",
      "195\n",
      "665\n",
      "553\n",
      "400\n",
      "672\n",
      "352\n",
      "680\n",
      "44\n",
      "31\n",
      "623\n",
      "526\n",
      "61\n",
      "695\n",
      "185\n",
      "633\n",
      "318\n",
      "496\n",
      "188\n",
      "119\n",
      "605\n",
      "149\n",
      "2\n",
      "482\n",
      "519\n",
      "377\n",
      "290\n",
      "15\n",
      "706\n",
      "685\n",
      "97\n",
      "469\n",
      "445\n",
      "242\n",
      "705\n",
      "696\n",
      "152\n",
      "689\n",
      "76\n",
      "360\n",
      "314\n",
      "512\n",
      "658\n",
      "163\n",
      "537\n",
      "5\n",
      "264\n",
      "520\n",
      "576\n",
      "538\n",
      "497\n",
      "187\n",
      "614\n",
      "708\n",
      "258\n",
      "660\n",
      "165\n",
      "501\n",
      "659\n",
      "12\n",
      "632\n",
      "174\n",
      "499\n",
      "714\n",
      "186\n",
      "224\n",
      "250\n",
      "41\n",
      "607\n",
      "515\n",
      "644\n",
      "473\n",
      "220\n",
      "697\n",
      "235\n",
      "667\n",
      "698\n",
      "676\n",
      "670\n",
      "233\n",
      "630\n",
      "252\n",
      "451\n",
      "629\n",
      "702\n",
      "524\n",
      "29\n",
      "206\n",
      "693\n",
      "690\n",
      "52\n",
      "581\n",
      "267\n",
      "573\n",
      "600\n",
      "2\n",
      "359\n",
      "420\n",
      "472\n",
      "475\n",
      "508\n",
      "502\n",
      "383\n",
      "56\n",
      "417\n",
      "509\n",
      "345\n",
      "539\n",
      "485\n",
      "260\n",
      "91\n",
      "96\n",
      "585\n",
      "394\n",
      "427\n",
      "313\n",
      "493\n",
      "513\n",
      "339\n",
      "478\n",
      "432\n",
      "393\n",
      "90\n",
      "440\n",
      "416\n",
      "480\n",
      "147\n",
      "562\n",
      "179\n",
      "550\n",
      "384\n",
      "35\n",
      "575\n",
      "161\n",
      "668\n",
      "561\n",
      "484\n",
      "100\n",
      "203\n",
      "330\n",
      "370\n",
      "525\n",
      "214\n",
      "412\n",
      "230\n",
      "207\n",
      "95\n",
      "428\n",
      "123\n",
      "422\n",
      "586\n",
      "707\n",
      "588\n",
      "506\n",
      "568\n",
      "11\n",
      "565\n",
      "244\n",
      "62\n",
      "346\n",
      "390\n",
      "172\n",
      "476\n",
      "72\n",
      "531\n",
      "239\n",
      "418\n",
      "534\n",
      "406\n",
      "217\n",
      "500\n",
      "363\n",
      "409\n",
      "121\n",
      "3\n",
      "628\n",
      "326\n",
      "372\n",
      "127\n",
      "132\n",
      "366\n",
      "37\n",
      "510\n",
      "198\n",
      "495\n",
      "343\n",
      "516\n",
      "387\n",
      "430\n",
      "378\n",
      "567\n",
      "577\n",
      "584\n",
      "144\n",
      "364\n",
      "109\n",
      "371\n",
      "249\n",
      "556\n",
      "657\n",
      "604\n",
      "713\n",
      "699\n",
      "595\n",
      "381\n",
      "303\n",
      "344\n",
      "332\n",
      "205\n",
      "486\n",
      "282\n",
      "529\n",
      "592\n",
      "522\n",
      "148\n",
      "275\n",
      "459\n",
      "489\n",
      "635\n",
      "295\n",
      "306\n",
      "156\n",
      "488\n",
      "309\n",
      "226\n",
      "228\n",
      "542\n",
      "292\n",
      "491\n",
      "545\n",
      "65\n",
      "642\n",
      "709\n",
      "691\n",
      "464\n",
      "66\n",
      "3\n",
      "112\n",
      "606\n",
      "458\n",
      "688\n",
      "14\n",
      "404\n",
      "474\n",
      "470\n",
      "310\n",
      "265\n",
      "262\n",
      "617\n",
      "523\n",
      "425\n",
      "246\n",
      "160\n",
      "266\n",
      "87\n",
      "648\n",
      "201\n",
      "167\n",
      "68\n",
      "336\n",
      "36\n",
      "141\n",
      "483\n",
      "358\n",
      "441\n",
      "375\n",
      "671\n",
      "287\n",
      "498\n",
      "402\n",
      "173\n",
      "118\n",
      "129\n",
      "83\n",
      "546\n",
      "135\n",
      "181\n",
      "487\n",
      "328\n",
      "227\n",
      "154\n",
      "391\n",
      "431\n",
      "350\n",
      "541\n",
      "490\n",
      "468\n",
      "517\n",
      "413\n",
      "105\n",
      "245\n",
      "81\n",
      "196\n",
      "423\n",
      "210\n",
      "124\n",
      "424\n",
      "86\n",
      "94\n",
      "636\n",
      "140\n",
      "175\n",
      "241\n",
      "615\n",
      "540\n",
      "679\n",
      "232\n",
      "59\n",
      "111\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'heat_map_visualization_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(books_info)        \n\u001b[1;32m     40\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/results/top2vec/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(folder, save_name))\n\u001b[0;32m---> 42\u001b[0m heat_mat_3 \u001b[38;5;241m=\u001b[39m \u001b[43mheat_map_visualization_3\u001b[49m(df)\n\u001b[1;32m     43\u001b[0m plot_heat_map(heat_mat_3, df, save_name, \u001b[38;5;241m3\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     45\u001b[0m heat_mat_2 \u001b[38;5;241m=\u001b[39m heat_map_visualization_2(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'heat_map_visualization_3' is not defined"
     ]
    }
   ],
   "source": [
    "for embedding_name in [\"doc2vec\"]: #, 'doc2vec','paraphrase-multilingual-MiniLM-L12-v2', 'distiluse-base-multilingual-cased'\n",
    "    for u_map_n_neighbours in range(15,16, 5):\n",
    "        umap_args = {'n_neighbors':u_map_n_neighbours, \n",
    "                      'n_components':5, \n",
    "                      'metric':'cosine', \n",
    "                      'random_state':42}\n",
    "        for hdbscan_min_cluster_size in range(15, 16,  5):\n",
    "            \n",
    "            hdbscan_args = {'min_cluster_size':hdbscan_min_cluster_size, \n",
    "                            'metric':'euclidean', \n",
    "                            'cluster_selection_method':'eom', \n",
    "                            'prediction_data':True}\n",
    "            \n",
    "            topic_model = Top2Vec(documents = books, umap_args=umap_args, hdbscan_args=hdbscan_args, speed='fast-learn',embedding_model= embedding_name, workers = 8) #\n",
    "            save_name = 'without_cs_top2vec_umap_n_n{}_hdbscan_m_c_s{}_{}_new{}'.format(str(u_map_n_neighbours), str(hdbscan_min_cluster_size), str(embedding_name), file_ending)\n",
    "            #save_name = 'top2vec_original_setting_new_NOV_COL_VER'\n",
    "            #topic_model = Top2Vec.load('models/whole books/top2vec/{}'.format(save_name))\n",
    "            topic_model.save('models/{}/top2vec/{}'.format(folder, save_name))\n",
    "            num_topics = topic_model.get_num_topics()\n",
    "\n",
    "            topic_words, word_scores, topics = topic_model.get_topics(num_topics=num_topics) # Specify the number of topics \n",
    "            \n",
    "            df_topic_words = pd.DataFrame({\"topic {i}\".format(i=i):topic_words[i, :] for i in range(num_topics)}) #{\"topic 1\":topic_words[0, :], \"topic 2\":topic_words[1, :]}\n",
    "            df_topic_words.to_excel(\"data/topic words/top2vec/{}/{}.xlsx\".format(folder, save_name))     \n",
    "            \n",
    "            \n",
    "            topic_sizes, topic_nums = topic_model.get_topic_sizes()\n",
    "            doc_top_reduced = [0] * (len(books_info))\n",
    "            for topic_num in range(num_topics): \n",
    "                print(topic_num)\n",
    "                documents, document_scores, document_ids = topic_model.search_documents_by_topic(topic_num=topic_num, num_docs=topic_sizes[topic_num])\n",
    "                for document_id, document_score in zip(document_ids, document_scores):\n",
    "                    print(document_id)\n",
    "                    doc_top_reduced[document_id] = topic_num \n",
    "                    books_info[document_id]['topic'] = topic_num \n",
    "                    books_info[document_id]['score'] = document_score\n",
    "\n",
    "            df = pd.DataFrame(books_info)        \n",
    "\n",
    "            df.to_excel('data/results/top2vec/{}/{}.xlsx'.format(folder, save_name))\n",
    "\n",
    "            heat_mat_3 = heat_map_visualization_3(df)\n",
    "            plot_heat_map(heat_mat_3, df, save_name, 3, ['cz', 'en', 'other'])\n",
    "\n",
    "            heat_mat_2 = heat_map_visualization_2(df)\n",
    "            plot_heat_map(heat_mat_2, df, save_name, 2, ['cz', 'trans'])\n",
    "            #plot_top2vec(u_map_n_neighbours, topic_model, save_name, doc_top_reduced)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE TOPIC WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_TF_IDF(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    #for ele1, ele2 in zip(vectorizer.get_feature_names_out(), vectorizer.idf_):\n",
    "    #    print(ele1, ':', ele2)\n",
    "    return X.to_array()\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_name in [\"doc2vec\"]: #, 'doc2vec','paraphrase-multilingual-MiniLM-L12-v2', 'distiluse-base-multilingual-cased'\n",
    "    for u_map_n_neighbours in range(15,16, 5):\n",
    "        umap_args = {'n_neighbors':u_map_n_neighbours, \n",
    "                      'n_components':5, \n",
    "                      'metric':'cosine', \n",
    "                      'random_state':42}\n",
    "        for hdbscan_min_cluster_size in range(15, 16,  5):\n",
    "            \n",
    "            hdbscan_args = {'min_cluster_size':hdbscan_min_cluster_size, \n",
    "                            'metric':'euclidean', \n",
    "                            'cluster_selection_method':'eom', \n",
    "                            'prediction_data':True}\n",
    "            \n",
    "            save_name = 'top2vec_umap_n_n{}_hdbscan_m_c_s{}_{}_new'.format(str(u_map_n_neighbours), str(hdbscan_min_cluster_size), str(embedding_name)) #_NOV_COL_MEM_VER\n",
    "            #save_name = 'top2vec_original_setting_new_NOV_COL_MEM_VER'\n",
    "            #topic_model = Top2Vec.load('models/whole books/top2vec/{}'.format(save_name))\n",
    "            topic_model = Top2Vec.load('models/whole books/top2vec/{}'.format(save_name)) #  NOV COL MEM VER\n",
    "            num_topics = topic_model.get_num_topics()\n",
    "\n",
    "            topic_words, word_scores, topics = topic_model.get_topics(num_topics=num_topics) # Specify the number of topics you want\n",
    "\n",
    "            df_topic_words = pd.DataFrame({\"topic {i}\".format(i=i):topic_words[i, :] for i in range(num_topics)}) #{\"topic 1\":topic_words[0, :], \"topic 2\":topic_words[1, :]}\n",
    "            df_topic_words.to_excel(\"data/topic words/top2vec/whole books/{}.xlsx\".format(save_name))   #  NOV COL MEM VER   \n",
    "            \n",
    "            \n",
    "            topic_sizes, topic_nums = topic_model.get_topic_sizes()\n",
    "            doc_top_reduced = [0] * (len(books_info))\n",
    "            for topic_num in range(num_topics): \n",
    "                print(topic_num)\n",
    "                documents, document_scores, document_ids = topic_model.search_documents_by_topic(topic_num=topic_num, num_docs=topic_sizes[topic_num])\n",
    "                tf_idf_vector = get_TF_IDF(documents)\n",
    "                print(tf_idf_vector)\n",
    "                for document_id, document_score in zip(document_ids, document_scores):\n",
    "                    print(document_id)\n",
    "                    doc_top_reduced[document_id] = topic_num \n",
    "                    books_info[document_id]['topic'] = topic_num \n",
    "                    books_info[document_id]['score'] = document_score\n",
    "                    \n",
    "\n",
    "            df = pd.DataFrame(books_info)        \n",
    "\n",
    "            #df.to_excel('data/results/top2vec/whole books  NOV COL MEM VER/{}.xlsx'.format(save_name))\n",
    "\n",
    "            #heat_mat_3 = heat_map_visualization_3(df)\n",
    "            #plot_heat_map(heat_mat_3, df, save_name, 3, ['cz', 'en', 'other'])\n",
    "\n",
    "            #heat_mat_2 = heat_map_visualization_2(df)\n",
    "            #plot_heat_map(heat_mat_2, df, save_name, 2, ['cz', 'trans'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET X for RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words = get_stop_words('czech')\n",
    "#stop_words.extend(['aby', 'být'])\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "X_structural = {'ids': [], 'tfidf' : [], 'pos': [], 'avg_sentence_length': [], 'number_of_sentences':[], 'topic': [] }\n",
    "\n",
    "documents_list = []\n",
    "for embedding_name in [\"doc2vec\"]: #, 'doc2vec','paraphrase-multilingual-MiniLM-L12-v2', 'distiluse-base-multilingual-cased'\n",
    "    for u_map_n_neighbours in range(15,16, 5):\n",
    "        umap_args = {'n_neighbors':u_map_n_neighbours, \n",
    "                      'n_components':5, \n",
    "                      'metric':'cosine', \n",
    "                      'random_state':42}\n",
    "        \n",
    "        for hdbscan_min_cluster_size in range(15, 16,  5):\n",
    "            \n",
    "            tf_idf_vectors = []\n",
    "            hdbscan_args = {'min_cluster_size':hdbscan_min_cluster_size, \n",
    "                            'metric':'euclidean', \n",
    "                            'cluster_selection_method':'eom', \n",
    "                            'prediction_data':True}\n",
    "            \n",
    "            save_name = 'without_cs_top2vec_umap_n_n{}_hdbscan_m_c_s{}_{}_new{}'.format(str(u_map_n_neighbours), str(hdbscan_min_cluster_size), str(embedding_name), file_ending)\n",
    "            #save_name = 'top2vec_original_setting_new_NOV_COL_VER'\n",
    "            #topic_model = Top2Vec.load('models/whole books/top2vec/{}'.format(save_name))\n",
    "            topic_model = Top2Vec.load('models/{}/top2vec/{}'.format(folder, save_name)) #  NOV COL MEM VER\n",
    "            num_topics = topic_model.get_num_topics()\n",
    "            \n",
    "            topic_sizes, topic_nums = topic_model.get_topic_sizes()\n",
    "            for topic_num in range(num_topics): \n",
    "                print(topic_num)\n",
    "                documents, document_scores, document_ids = topic_model.search_documents_by_topic(topic_num=topic_num, num_docs=topic_sizes[topic_num])\n",
    "                documents = [' '.join([x for x in document.split()]) for document in documents.tolist()] #if x not in stop_words else ''  <-- get rid of stop words\n",
    "                documents_list.append(' '.join(documents))\n",
    "                X.extend(documents)\n",
    "                y.extend([topic_num for x in range(len(documents))])\n",
    "                #print(tf_idf_vector)\n",
    "\n",
    "               \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tf_idf = []\n",
    "for i, l in enumerate(tf_idf_vectors):\n",
    "    all_but_l = sum(tf_idf_vectors[:i] + tf_idf_vectors[i+1:], [])\n",
    "    filtered = list(set(l).difference(set(all_but_l)))\n",
    "    filtered_tf_idf.append(filtered) \n",
    "    print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÜMIT RANDOMFOREST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['změknout', 'ucucnout', 'čistota', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['rozhodování', 'zodpovídat', 'učení', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['totožný', 'urputnost', 'soutěživost', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['stísněně', 'psaný', 'urovnat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['uvolněně', 'atlas', 'kapesní', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['obezřetně', 'přirůst', 'znávat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['malina', 'baže', 'péci', ..., 'jít', 'být', 'pak'], dtype='<U50'), array(['uveřejnění', 'dlaha', 'znehybněný', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['odmlčal', 'breptat', 'pff', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['vyvolání', 'vycpávat', 'zbavený', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['pompidou', 'miniatura', 'účelový', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['tetovaný', 'skóre', 'uschovat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['nemravnost', 'dán', 'jstli', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['důvodný', 'rozrušovat', 'vojín', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['papírovat', 'kapitánův', 'párátko', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['čtyřčlenný', 'staveništní', 'knox', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['ovlivňovat', 'nedbalost', 'rezignovat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['rozkmotřit', 'svolit', 'prodloužení', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['debakl', 'laminát', 'vykouzlit', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['kutit', 'kňourání', 'násilně', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['vě', 'vypočítaný', 'sunoucí', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['kotvívat', 'lidožrout', 'repetent', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['pečetit', 'rostlinka', 'dolárek', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zadřít', 'pouštění', 'osvobozený', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['parviner', 'dobytý', 'zahození', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['karcinom', 'bronchogenický', 'vískat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['jakživ', 'benefit', 'rýma', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['promrznout', 'zprostředkovací', 'mantillou', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['zchrastit', 'upracovaný', 'bůhsuď', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zaniknout', 'odnaučit', 'rez', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zhnusení', 'veslování', 'pólo', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['vně', 'zaco', 'odpáraný', ..., 'jít', 'být', 'pak'], dtype='<U50'), array(['zadělat', 'sářin', 'chateau', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['luminiscenční', 'vestavět', 'nonstop', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zpitomělý', 'zapotácení', 'shromažďování', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['přiléhat', 'favorit', 'vyzývatel', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['táflování', 'třepotavý', 'černorůžový', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['mohovitě', 'malomocenství', 'donucování', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['viržinka', 'velmoc', 'masakrovat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['akvaristika', 'filatelie', 'smetáček', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zamumalala', 'fangle', 'promenádovat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['podjíždět', 'alžírský', 'leklý', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['postkoitální', 'anarchista', 'vyklepávat', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['chlapisko', 'předpisovat', 'přihrbit', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['nezkrotně', 'počůraný', 'rákoska', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['pololetní', 'kasař', 'tepénka', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['pětašedesátiletý', 'ellior', 'columbijský', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['nezdvořáctví', 'rozjaření', 'přinesení', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['škodný', 'vercajk', 'povyhrnuje', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['tralala', 'plesk', 'odhadování', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['konektor', 'otrokářka', 'pamětnický', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['otcovsky', 'propadnutí', 'semknutý', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['šmik', 'vvykřikl', 'medúza', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['monočlánek', 'probíhlo', 'gardiner', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['ošoust', 'šestadevadesátý', 'rozčleněný', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['pruderie', 'svobodomyslně', 'suverénnost', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['beranidlo', 'výklopný', 'ádný', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['preparovat', 'obalky', 'čtvrtdolarová', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['benzodiazepam', 'seder', 'trumbera', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['sedmivrstevného', 'holmesův', 'limča', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['žebrota', 'uskrovnit', 'omylný', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['hazardovat', 'jelení', 'smečovat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['odechla', 'kuvasz', 'osmóza', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['uvnitřní', 'neotesaný', 'dannin', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['vypadale', 'sešeřit', 'chroupavý', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['prostovlasý', 'kristaboha', 'přísavka', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['činívat', 'zrůzněný', 'odloudit', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zainteresovat', 'poškubávající', 'vypiplaný', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['družice', 'telekomunikační', 'distinkce', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['vykrákat', 'zapřemítat', 'zablikotat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['pojištovák', 'zporážet', 'čísnout', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['esdéčka', 'profláknutý', 'epigram', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['křepelák', 'landrover', 'vrstvit', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['odlakovávat', 'sentence', 'vetřelkyně', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['oscilace', 'ničící', 'vyslání', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['basketballu', 'hejsalo', 'wanderlust', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['vlastizrádný', 'přemítavý', 'nakomandovat', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['vii', 'lam', 'dittmar', 'zavrkat', 'carson', 'produkující',\n",
      "       'penologie', 'respektování', 'vynucení', 'nevadský', 'hroch',\n",
      "       'pohonný', 'bernard', 'automobilka', 'zlovolný', 'projektil',\n",
      "       'pouvažovat', 'naleziště', 'rozmach', 'citrus', 'ostýchavý',\n",
      "       'vybraně', 'odkázaný', 'předběžně', 'rozepře', 'montážní',\n",
      "       'záporně', 'málokdo', 'zemědělský', 'distributor', 'výstavba',\n",
      "       'usmrcení', 'výrok', 'spáchaný', 'porotce', 'konzultace',\n",
      "       'námluvy', 'lékařství', 'čerpací', 'zmíněný', 'nápravný',\n",
      "       'spravovat', 'hmota', 'expert', 'políbení', 'fešanda', 'rozběsnit',\n",
      "       'benzinový', 'pumpa', 'využití', 'opatrovník', 'obžaloba', 'obor',\n",
      "       'použití', 'realitní', 'smrtící', 'čtenář', 'rozpor', 'správce',\n",
      "       'odebrat', 'autor', 'letmý', 'okres', 'fair', 'projíždět',\n",
      "       'zahrnovat', 'usmívající', 'tisk', 'neohrabaný', 'křoví',\n",
      "       'pseudonym', 'předseda', 'rozloha', 'trestný', 'ihned',\n",
      "       'zavraždění', 'usvědčit', 'brutální', 'znásilnit', 'devětadvacet',\n",
      "       'strážce', 'komplex', 'učinit', 'účel', 'státní', 'městský',\n",
      "       'zvolit', 'zážitek', 'aktuální', 'násilník', 'milostný',\n",
      "       'informovat', 'úmysl', 'prozkoumat', 'překonat', 'zařízení',\n",
      "       'instinkt', 'spontánní', 'naděje', 'vděčný', 'vypovědět', 'poučit',\n",
      "       'vášeň', 'osudný', 'nyní', 'prudký', 'pracující', 'odlehlý',\n",
      "       'kapitola', 'zrak', 'vyzvat', 'potěšení', 'bytost', 'vyřešit',\n",
      "       'postižený', 'věznice', 'domnívat', 'spolehnout', 'osobnost',\n",
      "       'užít', 'období', 'konečně', 'milý', 'rada', 'naživu', 'mohutný',\n",
      "       'anonymní', 'osobní', 'dodat', 'odpor', 'vrah', 'soudní',\n",
      "       'průmysl', 'soudce', 'vyvléknout', 'pozemek', 'potřebný',\n",
      "       'falešný', 'železnice', 'cit', 'obchodní', 'spravedlnost',\n",
      "       'potřást', 'majetek', 'souhlasit', 'násilí', 'dokazovat',\n",
      "       'výdělek', 'dotyčný', 'fakticky', 'kampaň', 'předvolební',\n",
      "       'tenkrát', 'chlapík', 'věnovat', 'činnost', 'naléhat', 'odborník',\n",
      "       'lidský', 'nutit', 'odmítnout', 'odchod', 'podmínka', 'vyslovit',\n",
      "       'továrna', 'část', 'prohlížet', 'závěr', 'dospět', 'patřičný',\n",
      "       'pátrat', 'souviset', 'zákon', 'pohovořit', 'zločinnost',\n",
      "       'připravit', 'centrum', 'hovořit', 'protizákonný', 'uvědomovat',\n",
      "       'určit', 'uniknout', 'vyšetřování', 'pokusit', 'vypátrat',\n",
      "       'špetka', 'chladný', 'míra', 'společník', 'skutečný', 'vhodný',\n",
      "       'náhlý', 'informace', 'schopný', 'prezident', 'znamenat', 'síla',\n",
      "       'porada', 'zatáhnout', 'povědět', 'pán', 'výkon', 'zavřený',\n",
      "       'možnost', 'jakmile', 'anebo', 'upřít', 'smět', 'porota',\n",
      "       'poskytnout', 'akce', 'příliš', 'zda', 'přístup', 'tisíc', 'banka',\n",
      "       'možný', 'naleznout', 'ovšem', 'přiznat', 'sedm', 'cena', 'ano',\n",
      "       'zabití', 'upozornit', 'finanční', 'záležet', 'odměna', 'úplně',\n",
      "       'vzájemný', 'navrhnout', 'zajímavý', 'člen', 'příspěvek', 'forma',\n",
      "       'patnáct', 'přítel', 'muž', 'šest', 'chovat', 'vůz', 'byt',\n",
      "       'počkat', 'rozhovor', 'pětatřicet', 'tvrdit', 'obvinění', 'soud',\n",
      "       'mladý', 'náš', 'zavěsit', 'roztomilý', 'proto', 'grove', 'silný',\n",
      "       'hlavní', 'deset', 'rána', 'políbit', 'pokoušet', 'kniha', 'hlas',\n",
      "       'názor', 'zbraň', 'oblast', 'tento', 'prodej', 'východ', 'stanice',\n",
      "       'přemýšlet', 'takže', 'zabít', 'znít', 'komora', 'pravděpodobně',\n",
      "       'kolem', 'podařit', 'hodina', 'policie', 'začít', 'snadný',\n",
      "       'devátý', 'západ', 'sdělit', 'pohlédnout', 'opatrně', 'malý',\n",
      "       'nový', 'zpráva', 'psát', 'číst', 'slovo', 'potřebovat', 'teprve',\n",
      "       'určitě', 'čin', 'místo', 'docela', 'majitel', 'také', 'najít',\n",
      "       'bez', 'zeptat', 'čtyři', 'opustit', 'zůstat', 'kancelář', 'další',\n",
      "       'můj', 'jejich', 'případ', 'sám', 'ani', 'obsluhovat', 'dolar',\n",
      "       'hlava', 'protože', 'poté', 'minuta', 'pod', 'čekat', 'jiný', 'už',\n",
      "       'zavolat', 'brzy', 'dít', 'všelijaký', 'město', 'slečna', 'podnik',\n",
      "       'dostat', 'země', 'takový', 'snažit', 'až', 'všechen',\n",
      "       'připomínat', 'rád', 'vypadat', 'sedět', 'pravda', 'přijít',\n",
      "       'hezký', 'vy', 'kdyby', 'stát', 'přísahat', 'dokázat', 'jaký',\n",
      "       'brát', 'noc', 'dnes', 'dovědět', 'dokud', 'než', 'vyjít',\n",
      "       'nakonec', 'odejít', 'představit', 'někdo', 'hledat', 'chtít',\n",
      "       'pro', 'tam', 'snad', 'podívat', 'dobrý', 'muset', 'po', 'rok',\n",
      "       'několik', 'před', 'tenhle', 'právě', 'aby', 'tak', 'velký',\n",
      "       'její', 'udělat', 'on', 'od', 'dva', 'však', 'žádný', 'svědek',\n",
      "       'nic', 'zločin', 'co', 'říci', 'celý', 'nebo', 'jeden', 'jen',\n",
      "       'moci', 'ale', 'vědět', 'když', 'teď', 'ne', 'já', 'podle', 'že',\n",
      "       'jestli', 'zjistit', 'důvod', 'ten', 'mít', 'jako', 'něco',\n",
      "       'který', 'žena', 'nějaký', 'na', 'dívat', 'svůj', 'do', 'se', 'za',\n",
      "       'jít', 'být', 'pak'], dtype='<U50'), array(['inon', 'tázací', 'meteorický', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zemina', 'trapezisté', 'proskákat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['živelný', 'kramařit', 'podsazení', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['doplétat', 'porážení', 'odpředu', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['magika', 'laterna', 'derick', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['etiopský', 'somálsko', 'macdonaldovský', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['popatnácté', 'vláčnost', 'nezdravo', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['cerdo', 'membrilly', 'barra', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['slavjanofil', 'rusofil', 'letý', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['laxní', 'vypakovávat', 'zásadově', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zakaboněle', 'škvrně', 'zafňukání', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['jarový', 'masturbant', 'ostřihat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['doping', 'olifanti', 'pentelowovi', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['doběh', 'troubelí', 'makrónkovou', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['wicca', 'barmský', 'chronicky', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['znetvořovat', 'stať', 'extrémista', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['damašek', 'lichotný', 'samson', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['scházívat', 'zmršený', 'mláďátko', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['sidcupského', 'pozamykat', 'nadraka', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['forota', 'par', 'netový', ..., 'jít', 'být', 'pak'], dtype='<U50'), array(['tyknout', 'plátkový', 'škrundat', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['kličkující', 'rozmáčknutý', 'uzené', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['propletenina', 'budweiserů', 'kasaštyk', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['výpravčí', 'přibouchávat', 'vydusaný', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['blouznivec', 'stray', 'panikářský', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['natahující', 'levitačními', 'wunderlandskou', ..., 'jít', 'být',\n",
      "       'pak'], dtype='<U50'), array(['muezzin', 'žaluplný', 'střižení', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['rámování', 'odhodlaním', 'leopardice', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['vaúúúú', 'nééééé', 'přebývání', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['obrůst', 'šestiúhelník', 'republican', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['povídavost', 'tommmy', 'dutohlavý', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50'), array(['zaslepenost', 'domina', 'galle', ..., 'jít', 'být', 'pak'],\n",
      "      dtype='<U50')]\n",
      "Class 0\n",
      "Class 1\n",
      "Class 2\n",
      "Class 3\n",
      "\n",
      "Class: 0\n",
      "jo: 0.003977417533984942\n",
      "prý: 0.003723538081891323\n",
      "street: 0.003707965232188373\n",
      "strávit: 0.003480097140433949\n",
      "vypadat: 0.0032920352114259566\n",
      "návrat: 0.0031474392054147192\n",
      "zavrtět: 0.0027584560379451425\n",
      "linka: 0.0026877105958474754\n",
      "sedlák: 0.0026342645288750617\n",
      "zadívat: 0.0025935654236841263\n",
      "brandy: 0.0024528437238522105\n",
      "pořádek: 0.002442214531347905\n",
      "pražský: 0.0022749231073276293\n",
      "zlý: 0.002180112711652255\n",
      "minuta: 0.002177471604155026\n",
      "potřást: 0.0021525801336373385\n",
      "nervózní: 0.0021513739276859886\n",
      "sluchátko: 0.0021281612052186766\n",
      "sexy: 0.002088867249560227\n",
      "bar: 0.0020843117498227587\n",
      "také: 0.0020831191173882164\n",
      "otočit: 0.002082200026185871\n",
      "bránit: 0.002067393896581943\n",
      "přikývnout: 0.0020480200232766157\n",
      "marně: 0.001965642497285462\n",
      "\n",
      "Class: 1\n",
      "republika: 0.007377814873220876\n",
      "pražský: 0.006876007095064836\n",
      "legitimace: 0.004459680200712766\n",
      "komunistický: 0.004311961975877883\n",
      "soudruh: 0.003743106669816227\n",
      "dovnitř: 0.00321086498031087\n",
      "brát: 0.003007055047580393\n",
      "slivovice: 0.0029049897154586383\n",
      "pracoviště: 0.002829576156224483\n",
      "kulturní: 0.0028194299882655814\n",
      "okresní: 0.0027942757891812804\n",
      "koruna: 0.002664829730079671\n",
      "razítko: 0.0026255093933934596\n",
      "kultura: 0.0025082645536357332\n",
      "dokázat: 0.002484507234879366\n",
      "brněnský: 0.002399974062320068\n",
      "míle: 0.0023082064242684838\n",
      "socialismus: 0.002262168176003994\n",
      "aby: 0.002090868786078021\n",
      "zvednout: 0.002055533136940574\n",
      "jakmile: 0.002035031235740377\n",
      "vary: 0.0019390187649227482\n",
      "fabrika: 0.0019263316309605107\n",
      "německý: 0.001882497411472545\n",
      "marně: 0.00184859464408031\n",
      "\n",
      "Class: 2\n",
      "hotel: 0.0033850409475257776\n",
      "řidič: 0.003110892869092711\n",
      "ulice: 0.002627389913782042\n",
      "pobízet: 0.002578792076192069\n",
      "peníze: 0.0025419359444794685\n",
      "kouzelný: 0.0025063622613215085\n",
      "scéna: 0.0024332918253840642\n",
      "král: 0.0023317161119250636\n",
      "objednat: 0.002300680273953151\n",
      "papír: 0.0022039862726851165\n",
      "adresa: 0.002141066497222015\n",
      "káva: 0.002132362536295908\n",
      "fungovat: 0.0020977975136287153\n",
      "schůzka: 0.0020935109155008486\n",
      "hlavní: 0.0020587489947721323\n",
      "moudrý: 0.002040932196190151\n",
      "osobní: 0.0020024470709185836\n",
      "americký: 0.001986917206384604\n",
      "potulovat: 0.001979546297777583\n",
      "doktor: 0.001909038503998799\n",
      "čaroděj: 0.0019003090292395958\n",
      "uvidět: 0.0018368355282927457\n",
      "spadnout: 0.0018161953627533628\n",
      "uslyšet: 0.0017733691257978564\n",
      "šíp: 0.0017693770770525264\n",
      "\n",
      "Class: 3\n",
      "lyceum: 0.003217833816982606\n",
      "milostný: 0.0029151694187282027\n",
      "vděčit: 0.0028388211312261424\n",
      "zbožňovat: 0.002580707342445129\n",
      "ubohý: 0.0025213113797189312\n",
      "vytušit: 0.0024265133611450986\n",
      "dětinský: 0.002298865367072767\n",
      "listoví: 0.0021733737620138814\n",
      "agonie: 0.0021352016611410037\n",
      "vášeň: 0.002120540766094034\n",
      "dvojaký: 0.002065400534995094\n",
      "choroba: 0.002013295885791313\n",
      "mrzký: 0.001985991173636214\n",
      "zapotřebí: 0.0019674518775875376\n",
      "ňadra: 0.001851734966672126\n",
      "chladnokrevnost: 0.0018284595718997818\n",
      "okázalý: 0.0017983336679059725\n",
      "pokořený: 0.001750753156772658\n",
      "zdřímnout: 0.0017491368759852681\n",
      "odolávat: 0.0017176183850452806\n",
      "půvab: 0.0016427492628634415\n",
      "laskání: 0.0016425955640713785\n",
      "něha: 0.0016347587886134493\n",
      "milánský: 0.001612298321568086\n",
      "mlčení: 0.00158570412517781\n",
      "oob score:               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95       645\n",
      "         1.0       0.00      0.00      0.00        72\n",
      "\n",
      "    accuracy                           0.90       717\n",
      "   macro avg       0.45      0.50      0.47       717\n",
      "weighted avg       0.81      0.90      0.85       717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/charlottepanuskova/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "n_gram = 1\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,n_gram) )),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# how can i see what the tokenizer is doing?\n",
    "tokenized = pipeline.named_steps['tfidf'].fit_transform(X)\n",
    "# show tokenized as text\n",
    "tokenized_text = pipeline.named_steps['tfidf'].inverse_transform(tokenized)\n",
    "print(tokenized_text[:110])\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Get feature names from TfidfVectorizer\n",
    "feature_names = pipeline.named_steps['tfidf'].get_feature_names_out()\n",
    "\n",
    "# Get feature importances by class\n",
    "importances_by_class = {}\n",
    "for class_index, class_label in enumerate(pipeline.named_steps['rf'].classes_):\n",
    "    print('Class {}'.format(class_label))\n",
    "    # Binary classification: one class vs rest\n",
    "    y_binary = (y == class_label)\n",
    "    rf_binary = RandomForestClassifier(oob_score=classification_report)\n",
    "    rf_binary.fit(pipeline.named_steps['tfidf'].transform(X), y_binary)\n",
    "    importances_by_class[class_label] = dict(\n",
    "        sorted(zip(feature_names, rf_binary.feature_importances_), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Print feature importances by class\n",
    "for class_label, importances in importances_by_class.items():\n",
    "    print(f\"\\nClass: {class_label}\")\n",
    "    for feature_name, importance in list(importances.items())[:25]:\n",
    "        print(f\"{feature_name}: {importance}\")\n",
    "\n",
    "# report classification score on the oob set\n",
    "print(f\"oob score: {rf_binary.oob_score_}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: 0\n",
      "\n",
      "Class: 1\n",
      "\n",
      "Class: 2\n",
      "\n",
      "Class: 3\n"
     ]
    }
   ],
   "source": [
    "df_dict_rf = {}\n",
    "for class_label, importances in importances_by_class.items():\n",
    "    print(f\"\\nClass: {class_label}\")\n",
    "    df_dict_rf[class_label] = list(importances.keys())[:50]\n",
    "\n",
    "df_random_forrest_topics = pd.DataFrame(df_dict_rf) \n",
    "df_random_forrest_topics.to_excel(\"data/topic words/top2vec/{}/random_forrest_n_gram_{}_with_sw_{}.xlsx\".format(folder, n_gram, save_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic code for topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer model\n",
      "Get words\n",
      "ctfidf_model\n",
      "transform\n"
     ]
    }
   ],
   "source": [
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=False, reduce_frequent_words=True)\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1,2)) \n",
    "\n",
    "print('Vectorizer model')\n",
    "X2 = vectorizer_model.fit_transform(documents_list)\n",
    "\n",
    "print('Get words')\n",
    "words = vectorizer_model.get_feature_names_out()\n",
    "\n",
    "print('ctfidf_model')\n",
    "ctfidf_model = ctfidf_model.fit(X2, multiplier=None)\n",
    "\n",
    "print('transform')\n",
    "c_tf_idf = ctfidf_model.transform(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From BERTopic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _top_n_idx_sparse(matrix, n: int) -> np.ndarray:\n",
    "        \"\"\" Return indices of top n values in each row of a sparse matrix\n",
    "\n",
    "        Retrieved from:\n",
    "            https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix\n",
    "\n",
    "        Arguments:\n",
    "            matrix: The sparse matrix from which to get the top n indices per row\n",
    "            n: The number of highest values to extract from each row\n",
    "\n",
    "        Returns:\n",
    "            indices: The top n indices per row\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        for le, ri in zip(matrix.indptr[:-1], matrix.indptr[1:]):\n",
    "            n_row_pick = min(n, ri - le)\n",
    "            values = matrix.indices[le + np.argpartition(matrix.data[le:ri], -n_row_pick)[-n_row_pick:]]\n",
    "            values = [values[index] if len(values) >= index + 1 else None for index in range(n)]\n",
    "            indices.append(values)\n",
    "        return np.array(indices)\n",
    "\n",
    "def _top_n_values_sparse(matrix, indices: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Return the top n values for each row in a sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "            matrix: The sparse matrix from which to get the top n indices per row\n",
    "            indices: The top n indices per row\n",
    "\n",
    "        Returns:\n",
    "            top_values: The top n scores per row\n",
    "        \"\"\"\n",
    "        top_values = []\n",
    "        for row, values in enumerate(indices):\n",
    "            scores = np.array([matrix[row, value] if value is not None else 0 for value in values])\n",
    "            top_values.append(scores)\n",
    "        return np.array(top_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KeyBERTInspired.extract_topics() missing 1 required positional argument: 'topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m topics \u001b[38;5;241m=\u001b[39m {label: values[:top_n_words] \u001b[38;5;28;01mfor\u001b[39;00m label, values \u001b[38;5;129;01min\u001b[39;00m topics\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m representation_model \u001b[38;5;241m=\u001b[39m KeyBERTInspired()\n\u001b[0;32m---> 17\u001b[0m topics \u001b[38;5;241m=\u001b[39m \u001b[43mrepresentation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_tf_idf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: KeyBERTInspired.extract_topics() missing 1 required positional argument: 'topics'"
     ]
    }
   ],
   "source": [
    "top_n_words = 50\n",
    "indices = _top_n_idx_sparse(c_tf_idf, top_n_words)\n",
    "scores = _top_n_values_sparse(c_tf_idf, indices)\n",
    "sorted_indices = np.argsort(scores, 1)\n",
    "indices = np.take_along_axis(indices, sorted_indices, axis=1)\n",
    "scores = np.take_along_axis(scores, sorted_indices, axis=1)\n",
    "\n",
    "# topics = {label: [(words[word_index], score)\n",
    "#                           if  score > 0 #word_index is not None and\n",
    "#                           else (\"\", 0.00001)\n",
    "#                           for word_index, score in zip(indices[label][::-1], scores[label][::-1])\n",
    "#                           ]\n",
    "#                   for label in range(num_topics)}\n",
    "# topics = {label: values[:top_n_words] for label, values in topics.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "representation_model = KeyBERTInspired()\n",
    "topics = representation_model.extract_topics(documents_list, c_tf_idf, topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KeyBERT Inspired "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [int(label) for label in sorted(list(topics.keys()))]\n",
    "\n",
    "\n",
    "topics = {\n",
    "            label: [\n",
    "                (words[word_index], score) if word_index is not None and score > 0 else (\"\", 0.00001)\n",
    "                for word_index, score in zip(indices[index][::-1], scores[index][::-1])\n",
    "            ]\n",
    "            for index, label in enumerate(labels)\n",
    "        }\n",
    "topics = {label: list(zip(*values[: 50]))[0] for label, values in topics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('se',\n",
       " 'být',\n",
       " 'ten',\n",
       " 'on',\n",
       " 'na',\n",
       " 'že',\n",
       " 'do',\n",
       " 'ale',\n",
       " 'já',\n",
       " 'mít',\n",
       " 'který',\n",
       " 'co',\n",
       " 'oni',\n",
       " 'když',\n",
       " 'tak',\n",
       " 'jako',\n",
       " 'svůj',\n",
       " 'všechen',\n",
       " 'být se',\n",
       " 'za',\n",
       " 'moci',\n",
       " 'už',\n",
       " 'aby být',\n",
       " 'aby',\n",
       " 'ty',\n",
       " 'jen',\n",
       " 'jak',\n",
       " 'říci',\n",
       " 'být ten',\n",
       " 'po',\n",
       " 'jeho',\n",
       " 'my',\n",
       " 'ten být',\n",
       " 'ještě',\n",
       " 'chtít',\n",
       " 'muset',\n",
       " 'vědět',\n",
       " 'jít',\n",
       " 'jeden',\n",
       " 'pán',\n",
       " 'ani',\n",
       " 'že být',\n",
       " 'se on',\n",
       " 'vidět',\n",
       " 'můj',\n",
       " 'pak',\n",
       " 'až',\n",
       " 'stát',\n",
       " 'než',\n",
       " 'člověk')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bertopics = pd.DataFrame(topics) \n",
    "df_bertopics.to_excel(\"data/topic words/top2vec/{}/bertopic_{}.xlsx\".format(folder, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = topic_model.get_num_topics()\n",
    "\n",
    "topic_words, word_scores, topics = topic_model.get_topics(num_topics=num_topics) # Specify the number of topics you want\n",
    "\n",
    "df_topic_words = pd.DataFrame({\"topic {i}\".format(i=i):topic_words[i, :] for i in range(num_topics)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7485327)\n",
      "0,   (0, 7485261)\t0.008974776719429589\n",
      "  (0, 7485258)\t0.008023700338373662\n",
      "  (0, 7485226)\t0.008974776719429589\n",
      "  (0, 7485225)\t0.008974776719429589\n",
      "  (0, 7485210)\t0.008974776719429589\n",
      "  (0, 7485199)\t0.008974776719429589\n",
      "  (0, 7485180)\t0.00867291096751175\n",
      "  (0, 7485167)\t0.008974776719429589\n",
      "  (0, 7485166)\t0.008974776719429589\n",
      "  (0, 7485165)\t0.008974776719429589\n",
      "  (0, 7485164)\t0.008974776719429589\n",
      "  (0, 7485161)\t0.008974776719429589\n",
      "  (0, 7485160)\t0.008974776719429589\n",
      "  (0, 7485157)\t0.008974776719429589\n",
      "  (0, 7485148)\t0.008974776719429589\n",
      "  (0, 7485134)\t0.008974776719429589\n",
      "  (0, 7485133)\t0.008974776719429589\n",
      "  (0, 7485132)\t0.008474077079817684\n",
      "  (0, 7485131)\t0.008974776719429589\n",
      "  (0, 7485111)\t0.008974776719429589\n",
      "  (0, 7485110)\t0.008974776719429589\n",
      "  (0, 7485109)\t0.00867291096751175\n",
      "  (0, 7485100)\t0.008974776719429589\n",
      "  (0, 7485095)\t0.008974776719429589\n",
      "  (0, 7485091)\t0.007459053289752938\n",
      "  :\t:\n",
      "  (0, 320)\t0.008974776719429589\n",
      "  (0, 319)\t0.008974776719429589\n",
      "  (0, 300)\t0.008974776719429589\n",
      "  (0, 299)\t0.008974776719429589\n",
      "  (0, 298)\t0.008974776719429589\n",
      "  (0, 296)\t0.014043925068696795\n",
      "  (0, 232)\t0.008974776719429589\n",
      "  (0, 230)\t0.008474077079817684\n",
      "  (0, 216)\t0.008474077079817684\n",
      "  (0, 215)\t0.008108263918891027\n",
      "  (0, 212)\t0.008974776719429589\n",
      "  (0, 206)\t0.008974776719429589\n",
      "  (0, 201)\t0.010740552232387566\n",
      "  (0, 180)\t0.008974776719429589\n",
      "  (0, 178)\t0.0083255660902915\n",
      "  (0, 154)\t0.008974776719429589\n",
      "  (0, 153)\t0.008974776719429589\n",
      "  (0, 134)\t0.007884009467525753\n",
      "  (0, 127)\t0.008974776719429589\n",
      "  (0, 88)\t0.02213206451661861\n",
      "  (0, 52)\t0.011066032258309304\n",
      "  (0, 32)\t0.0071947199580647\n",
      "  (0, 28)\t0.008474077079817684\n",
      "  (0, 14)\t0.008974776719429589\n",
      "  (0, 12)\t0.023148438925689373\n"
     ]
    }
   ],
   "source": [
    "print(c_tf_idf[4].shape)\n",
    "len(words)\n",
    "label = 3\n",
    "top_n_words = 50\n",
    "indices = _top_n_idx_sparse(c_tf_idf, top_n_words)\n",
    "\n",
    "for en, score in enumerate(c_tf_idf[label][::-1]):\n",
    "    print('{}, {}'.format(en, score[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "u_map_n_neighbours = 15\n",
    "hdbscan_min_cluster_size = 15\n",
    "embedding_name = 'doc2vec'\n",
    "save_name = 'top2vec_umap_n_n{}_hdbscan_m_c_s{}_{}_new{}'.format(str(u_map_n_neighbours), str(hdbscan_min_cluster_size), str(embedding_name), file_ending)\n",
    "df_topic_results = pd.read_excel('data/results/top2vec/{}/{}.xlsx'.format(folder,save_name))\n",
    "df_info = pd.read_excel('data/books info/stats/jerome_all_books_info.xlsx')\n",
    "if fiction: df_info = df_info[df_info['txtype'].isin(txtype_select)].reset_index()\n",
    "df_info['topic'] = df_topic_results['topic']\n",
    "df_info['score'] = df_topic_results['score']\n",
    "pos_percentage = [{key: value/len(pos_tags) for key, value in Counter(pos_tags).items()} for pos_tags in pos.values()]# [{key: value/len(pos_tags)} for pos_tags in pos.values() for key, value in Counter(pos_tags).most_common() ]\n",
    "#df_info['pos'] = \n",
    "#df_info['pos'] = df_info['pos'].apply(lambda x: (key, value/len(x)) for key, value in x )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {'N': 'substantivum',\n",
    "            'A': 'adjektivum', \n",
    "            'P': 'pronomen',\n",
    "            'C': 'numerál', \n",
    "            'V': 'verbum',\n",
    "            'D': 'adverbium',\n",
    "            'R': 'prepozice',\n",
    "            'J': 'konjunkce',\n",
    "            'T': 'partikule',\n",
    "            'I': 'interjekce',\n",
    "            'S': 'segment',\n",
    "            'B': 'zkratka',\n",
    "            'Z': 'interpunkce', \n",
    "            'F': 'cizí slovo',\n",
    "            'X': 'neznámý'}\n",
    "\n",
    "def replace_string(string):\n",
    "    for key, value in pos_dict.items():\n",
    "        string = string.replace(key, value)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/charlottepanuskova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "# Texts divided into topic categories \n",
    "topic_texts = { 'id': [], 'documents': [], 'tokens':[], 'number_of_sentences': [], 'words': [], 'average_length_of_sentence': [], 'average_pos':[], 'topic': [], 'tfidf': [], 'pos_tfidf': [] }\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf_text = vectorizer.fit_transform(books).toarray()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "pos_list = [' '.join(value) for value in pos.values()]\n",
    "pos_list = [replace_string(value) for value in pos_list]\n",
    "\n",
    "X_tfidf_pos = vectorizer.fit_transform(pos_list).toarray()\n",
    "\n",
    "for i, row in df_info.iterrows():\n",
    "    document = books[i]\n",
    "    topic_texts['documents'].append(document)   \n",
    "    topic_texts['words'].append(row.words)\n",
    "    topic_texts['tokens'].append(row.tokens)\n",
    "    topic_texts['topic'].append(row.topic)\n",
    "    topic_texts['average_pos'].append(pos_percentage[i])\n",
    "    topic_texts['tfidf'].append(X_tfidf_text[i])\n",
    "    topic_texts['pos_tfidf'].append(X_tfidf_pos[i])\n",
    "    topic_texts['id'].append(i)\n",
    "    sentences = sent_tokenize(document)\n",
    "    words = word_tokenize(document)\n",
    "    topic_texts['number_of_sentences'] = len(sentences)\n",
    "    topic_texts['average_length_of_sentence'] = len(words) / len(sentences) \n",
    "\n",
    "topic_texts = pd.DataFrame(topic_texts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(topic_texts[['pos_tfidf']], topic_texts['topic']) #'pos_tfidf',  'tfidf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtopic_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumber_of_sentences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maverage_length_of_sentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maverage_pos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtfidf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "topic_texts[[\"tokens\", \"number_of_sentences\", \"average_length_of_sentence\", \"average_pos\", \"tfidf\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
